# Ollama Model Configuration
# Specify the Ollama model to use (e.g., llama2, llama3, mistral, codellama, qwen3)
# Note: Tool calling support requires a model that supports function calling (e.g., qwen3)
MODEL=llama2

# Ollama API Configuration (for streaming with tool calling)
# API key/token for Ollama authentication
# You can use either OLLAMA_API_KEY or OLLAMA_TOKEN (both are supported)
OLLAMA_API_KEY=your_ollama_api_key_here

# Ollama model to use for streaming with tool support
# This can be the same as MODEL above
OLLAMA_MODEL=qwen3
